{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 â€“ DataFrames and Grouping\n",
    "\n",
    "## DSC 80, Spring 2025\n",
    "\n",
    "### Due Date: Wednesday, April 16th at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to the second DSC 80 lab this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2024-fa).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.** More details on its usage are given at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>The only question in this lab that you're allowed to use a loop in is Question 3.</b> There, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame, but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tricky Pandas ðŸ¤”\n",
    "\n",
    "Sometimes, `pandas` gives you weird outputs that you may not expect. The next question walks you through a few examples that might surprise you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "The following subparts all require you to define a function and return a number that is the answer to a multiple-choice question. You may need to write code and experiment with DataFrames to arrive at your answers.\n",
    "\n",
    "#### `trick_me`\n",
    "\n",
    "`trick_me` should not take any arguments. \n",
    "<br>\n",
    "\n",
    "Inside the function:\n",
    "\n",
    "* Create a DataFrame named `tricky_1` that has three columns labeled `'Name'`, `'Name'`, and `'Age'`. `tricky_1` should have 5 rows; the values are up to you.\n",
    "* Save the DataFrame to a `.csv` file called `'tricky_1.csv'` without the index.\n",
    "* Now create another DataFrame, named `tricky_2`, by reading in the file `'tricky_1.csv'`. What are your observations?\n",
    "\n",
    "  1. It was not possible to create a DataFrame with the duplicate columns.\n",
    "  2. `tricky_1` and `tricky_2` have the same column names.\n",
    "  3. `tricky_1` and `tricky_2` have different column names.\n",
    "   \n",
    "Your function should return `1`, `2`, or `3`, answering the above question.\n",
    "\n",
    "<font color='red'>**Hints:** Dictionaries can only contain unique keys, so you <b>cannot</b> use them to declare a DataFrame with duplicate column names. Find other ways to create a DataFrame with duplicate column names.</font>\n",
    "\n",
    "<br>\n",
    "  \n",
    "#### `trick_bool`\n",
    "`trick_bool` should not take any arguments.\n",
    "\n",
    "To determine the correct answer from the list below, you should follow the steps outlined by experimenting in **the notebook** (or in the Terminal by running `python`). Outside the function:\n",
    "\n",
    "* Create a DataFrame named `bools` that has four columns: `True`, `True`, `False`, `False`. Each column name should be Boolean.\n",
    "* `bools` should have 4 rows; the values are up to you.\n",
    "* Predict the shape of the DataFrame that results by running each of the three lines of code below. Pick a corresponding answer from the given list. Your function should return a list with three numbers, one for each line.\n",
    "* You should be able to answer without running any code, but feel free to run code to check your answer.\n",
    "* **Your function should not do anything other than return a hardcoded list.**\n",
    "\n",
    "```py\n",
    "bools[True]\n",
    "bools[[True, True, False, False]]\n",
    "bools[[True, False]]\n",
    "```\n",
    "    \n",
    "Answer choices:\n",
    "1. DataFrame: 2 columns, 1 row\n",
    "2. DataFrame: 2 columns, 2 rows\n",
    "3. DataFrame: 2 columns, 3 rows\n",
    "4. DataFrame: 2 columns, 4 rows\n",
    "5. DataFrame: 3 columns, 1 rows\n",
    "6. DataFrame: 3 columns, 2 rows\n",
    "7. DataFrame: 3 columns, 3 rows\n",
    "8. DataFrame: 3 columns, 4 rows\n",
    "9. DataFrame: 4 columns, 1 rows\n",
    "10. DataFrame: 4 columns, 2 rows\n",
    "11. DataFrame: 4 columns, 3 rows\n",
    "12. DataFrame: 4 columns, 4 rows\n",
    "13. Error\n",
    "\n",
    "**Hints:** Refer to the previous hint for `trick_me`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tricky_1 and tricky_2 have different column names.\n"
     ]
    }
   ],
   "source": [
    "# create df \n",
    "col_names = ['Name','Name','Age']\n",
    "tricky_1 = (pd.DataFrame([['Brain','Mary',45],['Jen','Paul',15],['Kenny','Joseph',27],['Sue','Ally',65],['Aria','Harry',33]],\n",
    "                         columns=col_names))\n",
    "\n",
    "# tricky_1.iloc[:,2]\n",
    "tricky_1.to_csv('tricky_1.csv',index=False)\n",
    "\n",
    "try:\n",
    "    tricky_2 = pd.read_csv('tricky_1.csv')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to create DataFrame from CSV: {e}\")\n",
    "\n",
    "if tricky_2.columns.tolist() == tricky_1.columns.tolist():\n",
    "    print('tricky_1 and tricky_2 have the same column names')\n",
    "else:\n",
    "    print('tricky_1 and tricky_2 have different column names.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [True,True,False,False]\n",
    "bools = pd.DataFrame([[2,4,6,8],[3,5,7,9],[4,8,12,16],[5,10,15,20]],columns=col_names)\n",
    "\n",
    "# Boolean Mask Issue \n",
    "# passing a list of bools into column names bracket: df[[]] --> assumes we're selecting rows df[[rows],[columns]]\n",
    "# --> length of bool list must match length of row/column size bc it cant determine the values for the empty slots \n",
    "\n",
    "\n",
    "# bools[True].shape # (4,2)\n",
    "# fine because were finding the columns named True (bool)\n",
    "\n",
    "# bools[[True, True, False, False]].shape # (2,4)\n",
    "# boolean mask \n",
    "# length of bools list matches length of rows size \n",
    "# acts as bools.loc[[True, True, False, False]] therefore we are actually selecting the rows where we see True thus rows 0 and 1\n",
    "\n",
    "# bools[[True, False]] # error \n",
    "# boolean mask \n",
    "# ambiguous settings for the remaining 2/4 rows...dont know if they are True/False \n",
    "# if we want to select columns we would do bools.loc[:, [True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "trick_ans = trick_bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Summary Statistics ðŸ“Š\n",
    "\n",
    "In this question you will define two general purpose functions that make it easy to qualitatively assess the contents of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Complete the implementation of the function `population_stats`, which takes in a DataFrame `df` and returns a DataFrame indexed by the columns of `df`, with the following columns:\n",
    "* `'num_nonnull'`, which contains the number of non-null entries in each column.\n",
    "* `'prop_nonnull'`, which contains the proportion of entries in each column that are non-null.\n",
    "* `'num_distinct'`, which contains the number of distinct non-null entries in each column.\n",
    "* `'prop_distinct'`, which contains the proportion of non-null entries that are distinct in each column.\n",
    "       \n",
    "For example, if `df` has a column named `'ages'` with the following elements (note that `np.nan` is a null value):\n",
    "       \n",
    "```py\n",
    "[2, 2, 2, np.nan, 5, 7, 5, 10, 11, np.nan]\n",
    "```\n",
    "\n",
    "Then:\n",
    "- `'num_nonnull'` is 8, and `'prop_nonnull'` is $\\frac{8}{10}$ = 0.8.\n",
    "- There are six distinct entries, `[2, 5, 7, 10, 11, np.nan]`, but only 5 of them are non-null. So the number of distinct non-null entries, `'num_distinct'`, is 5.\n",
    "- There are 5 distinct non-null entries, and there are 8 total non-null entries, so `'prop_distinct'` is $\\frac{5}{8}$ = 0.625.\n",
    "\n",
    "Putting it all together, `population_stats(df).loc['ages']` should be a Series containing the numbers 8, 0.8, 5, and 0.625."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate random numbers\n",
    "value_pool = np.append(np.arange(30),np.nan)\n",
    "sampled_values = np.random.choice(value_pool, size=(100, 4), replace=True)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sampled_values, columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    "# start code\n",
    "new_df = pd.DataFrame(columns=['num_nonnull','prop_nonnull','num_distinct','prop_distinct'],index = df.T.index.tolist())\n",
    "\n",
    "numnotnull = df.notnull().sum().tolist()\n",
    "propnotnull = np.array(numnotnull) / df.shape[0]\n",
    "numdistinct = df.nunique().tolist()\n",
    "propdistinct = np.array(numdistinct) / np.array(numnotnull)\n",
    "\n",
    "new_df.iloc[:,0] = numnotnull\n",
    "new_df.iloc[:,1] = propnotnull\n",
    "new_df.iloc[:,2] = numdistinct\n",
    "new_df.iloc[:,3] = propdistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "pop_data = np.random.choice(range(10), size=(100, 4))\n",
    "df_pop = pd.DataFrame(pop_data, columns='A B C D'.split())\n",
    "out_pop = population_stats(df_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "    \n",
    "Complete the implementation of the function `most_common`, which takes in a DataFrame `df` and a number `N` and returns a DataFrame of the `N` most-common values and their counts for each column of `df`. Any column with fewer than `N` distinct values should contain `np.nan` in those entries.\n",
    "\n",
    "For example, consider the DataFrame shown on the left. This DataFrame is a subset of `salaries`, a larger DataFrame containing information on employees in the City of San Diego. The subset below contains two of the original columns: `'Job Title'` which contains job titles for employees, and `'status'` which denotes whether the employee works a full time position (`'FT'`) or a part time position (`'PT'`). On the right, the return value of `most_common(salaries, N=5)` is shown.\n",
    "\n",
    "You can assume that there are no ties in our hidden tests.\n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"data/imgs/dataframe.png\" width=\"90%\"/></td>\n",
    "    <td><img src=\"data/imgs/most_common.png\" width=\"90%\"/></td>\n",
    "</tr></table>\n",
    "\n",
    "***Note***: Remember, to access values in a Series based on their integer position, including when slicing for the first `N` values in a Series, you **must** use `.iloc` followed by square brackets. If you just use square brackets and don't use `.iloc`, you may not see the results you expect!\n",
    "\n",
    "***Hint***: You may find that initializing an empty DataFrame with `N` rows and adding columns to it is useful in your implementation.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Remember, the only question in this lab that you're allowed to use a loop in is Question 3 â€“ that's this question.</b> Here, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame (<code>df</code>), but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "common_data = np.random.choice(range(10), size=(100, 2))\n",
    "df = pd.DataFrame(common_data, columns='A B'.split())\n",
    "\n",
    "new_df = pd.DataFrame(index=np.arange(5))\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    name_col = df.columns[i] + '_values'\n",
    "    count_col = df.columns[i] + '_counts'\n",
    "\n",
    "    value_counts = df.iloc[:,i].value_counts()\n",
    "    # returns a Series with index: unique values and col: counts of the value\n",
    "\n",
    "    amount_nan = 5 - len(value_counts.index[:5])\n",
    "    if amount_nan > 0: # not enough distinct values \n",
    "        value_counts.index[:5] = value_counts.index[:5].tolist + [np.nan] * (amount_nan)\n",
    "        value_counts.values[:5] = value_counts.values[:5].tolist + [np.nan] * (amount_nan)\n",
    "\n",
    "    new_df = new_df.assign(**{name_col : value_counts.index[:5]})\n",
    "    new_df = new_df.assign(**{count_col : value_counts.values[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_values</th>\n",
       "      <th>A_counts</th>\n",
       "      <th>B_values</th>\n",
       "      <th>B_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_values  A_counts  B_values  B_counts\n",
       "0         4        20         8        16\n",
       "1         3        12         6        15\n",
       "2         0        11         0        12"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "common_data = np.random.choice(range(10), size=(100, 2))\n",
    "common_df = pd.DataFrame(common_data, columns='A B'.split())\n",
    "common_out = most_common(common_df, N=3)\n",
    "common_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Superheroes ðŸ¦¸\n",
    "\n",
    "The questions below analyze a dataset of superheroes found in the `data` directory. One of the datasets lists the attributes of each superhero, while the other is a *Boolean* DataFrame describing which superheroes have which superpowers. Note, the datasets contain information on both **good** superheroes, as well as **bad** superheroes (AKA villains).\n",
    "\n",
    "If you took DSC 10 in Fall 2022, this dataset may seem familiar â€“ it was used for the Final Project that quarter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Let's start working with the `powers` dataset, which you can see in `data/superheroes_powers.csv`. \n",
    "\n",
    "Complete the implementation of the function `super_hero_powers`, which takes in a DataFrame like `powers` and returns a list with the following three entries:\n",
    "\n",
    "1. The name of the superhero with the greatest number of superpowers.\n",
    "2. Identify the most common superpower among superheroes who can fly, other than `'Flight'` itself..\n",
    "3. The name of the most common superpower among superheroes with only one superpower.\n",
    "\n",
    "You should **not** be hard-coding your answers in this question; your function should work on any DataFrame similar to `powers`. In each case, you can assume the answer is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intelligence'"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# powers = powers.set_index('hero_names')\n",
    "\n",
    "# name_most_power = powers.sum(axis=1).sort_values(ascending=False).index[0] # sum of the rows --> add up all columns for each row  \n",
    "\n",
    "# after_flight = powers[powers['Flight']==True].sum(axis=0).sort_values(ascending=False).index[1] # sum of columns --> add up all rows for each col\n",
    "\n",
    "# # first: sum of the rows and filter df which superheroes have 1 power \n",
    "# # second: sum the columns and find the superpower that is most common among superheroes with only 1 power \n",
    "# name_one_power = powers[powers.sum(axis=1) == 1].sum(axis=0).sort_values(ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "super_fp = Path('data') / 'superheroes_powers.csv'\n",
    "powers = pd.read_csv(super_fp)\n",
    "super_out = super_hero_powers(powers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "In the notebook, load in the dataset in `data/superheroes.csv` as a DataFrame and explore it. Call your `population_stats` function from Question 2 on the DataFrame. You should notice that there are very few actually null (`np.nan`) values, but there are many entries that **should** be null, because they're missing.\n",
    "\n",
    "Complete the implementation of the function `clean_heroes`, which takes in a DataFrame like the one created from `superheroes.csv` and returns a new DataFrame with all of the missing values replaced with `np.nan`.\n",
    "\n",
    "After cleaning the superheroes dataset with `clean_heroes`, run `population_stats` on it again. As a result of the cleaning, population_stats should show that there are more null values.\n",
    "\n",
    "***Note***: Most of the work in this question is identifying how the missing values are stored in the DataFrame. The implementation of the function should only take one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Male', 'Female', '-'], dtype=object)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# superheroes_fp = Path('data') / 'superheroes.csv'\n",
    "# heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "# missing_str = ['-','Unknown','','None',-99.0]\n",
    "\n",
    "#heroes.replace(to_replace=missing_str,value=np.nan)\n",
    "\n",
    "# heroes['Weight'].unique() # 'nan', -99.0\n",
    "# heroes['Alignment'].unique() # '-'\n",
    "# heroes['Skin color'].unique() # '-'\n",
    "# heroes['Publisher'].unique() # 'nan'\n",
    "# heroes['Height'].unique() # clean\n",
    "# heroes['Hair color'].unique() # '-'\n",
    "# heroes['Race'].unique() # '-'\n",
    "# heroes['Eye color'].unique() # '-'\n",
    "# heroes['Gender'].unique() # '-'\n",
    "# heroes['name'].unique() # clean\n",
    "# new_heroes = heroes.replace(to_replace=missing_str,value=np.nan)\n",
    "\n",
    "# population_stats(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = Path('data') / 'superheroes.csv'\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "clean_out = clean_heroes(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have displayed the first 10 rows of the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Eye color</th>\n",
       "      <th>Race</th>\n",
       "      <th>Hair color</th>\n",
       "      <th>Height</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Skin color</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-Bomb</td>\n",
       "      <td>Male</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Human</td>\n",
       "      <td>No Hair</td>\n",
       "      <td>203.0</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "      <td>441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abe Sapien</td>\n",
       "      <td>Male</td>\n",
       "      <td>blue</td>\n",
       "      <td>Icthyo Sapien</td>\n",
       "      <td>No Hair</td>\n",
       "      <td>191.0</td>\n",
       "      <td>Dark Horse Comics</td>\n",
       "      <td>blue</td>\n",
       "      <td>good</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abin Sur</td>\n",
       "      <td>Male</td>\n",
       "      <td>blue</td>\n",
       "      <td>Ungaran</td>\n",
       "      <td>No Hair</td>\n",
       "      <td>185.0</td>\n",
       "      <td>DC Comics</td>\n",
       "      <td>red</td>\n",
       "      <td>good</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abomination</td>\n",
       "      <td>Male</td>\n",
       "      <td>green</td>\n",
       "      <td>Human / Radiation</td>\n",
       "      <td>No Hair</td>\n",
       "      <td>203.0</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bad</td>\n",
       "      <td>441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abraxas</td>\n",
       "      <td>Male</td>\n",
       "      <td>blue</td>\n",
       "      <td>Cosmic Entity</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bad</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Absorbing Man</td>\n",
       "      <td>Male</td>\n",
       "      <td>blue</td>\n",
       "      <td>Human</td>\n",
       "      <td>No Hair</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bad</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adam Monroe</td>\n",
       "      <td>Male</td>\n",
       "      <td>blue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blond</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NBC - Heroes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adam Strange</td>\n",
       "      <td>Male</td>\n",
       "      <td>blue</td>\n",
       "      <td>Human</td>\n",
       "      <td>Blond</td>\n",
       "      <td>185.0</td>\n",
       "      <td>DC Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Agent 13</td>\n",
       "      <td>Female</td>\n",
       "      <td>blue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blond</td>\n",
       "      <td>173.0</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Agent Bob</td>\n",
       "      <td>Male</td>\n",
       "      <td>brown</td>\n",
       "      <td>Human</td>\n",
       "      <td>Brown</td>\n",
       "      <td>178.0</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  Gender Eye color               Race Hair color  Height  \\\n",
       "0         A-Bomb    Male    yellow              Human    No Hair   203.0   \n",
       "1     Abe Sapien    Male      blue      Icthyo Sapien    No Hair   191.0   \n",
       "2       Abin Sur    Male      blue            Ungaran    No Hair   185.0   \n",
       "3    Abomination    Male     green  Human / Radiation    No Hair   203.0   \n",
       "4        Abraxas    Male      blue      Cosmic Entity      Black     NaN   \n",
       "5  Absorbing Man    Male      blue              Human    No Hair   193.0   \n",
       "6    Adam Monroe    Male      blue                NaN      Blond     NaN   \n",
       "7   Adam Strange    Male      blue              Human      Blond   185.0   \n",
       "8       Agent 13  Female      blue                NaN      Blond   173.0   \n",
       "9      Agent Bob    Male     brown              Human      Brown   178.0   \n",
       "\n",
       "           Publisher Skin color Alignment  Weight  \n",
       "0      Marvel Comics        NaN      good   441.0  \n",
       "1  Dark Horse Comics       blue      good    65.0  \n",
       "2          DC Comics        red      good    90.0  \n",
       "3      Marvel Comics        NaN       bad   441.0  \n",
       "4      Marvel Comics        NaN       bad     NaN  \n",
       "5      Marvel Comics        NaN       bad   122.0  \n",
       "6       NBC - Heroes        NaN      good     NaN  \n",
       "7          DC Comics        NaN      good    88.0  \n",
       "8      Marvel Comics        NaN      good    61.0  \n",
       "9      Marvel Comics        NaN      good    81.0  "
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Using the **cleaned** superhero data, we will now generate some insights.\n",
    "\n",
    "Complete the implementation of the function `super_hero_stats`, which takes no arguments and returns a list of length 6 containing your answers to the questions below. **Your answers should be hard-coded in the function.**\n",
    "\n",
    "0. What is the name of the tallest `'Mutant'` with `'No Hair'`?\n",
    "1. Among the publishers who have more than 5 characters, which publisher has the highest proportion of human characters? If there is a tie, return the publisher whose name is first alphabetically. We define a character to be human if their `'Race'` is exactly the string `'Human'`; for instance, a `'Race'` of `'Human / Radiation'` is non-human for the purposes of this question.\n",
    "2. Among the characters whose `'Height'`s we know, who is taller on average â€“ `'good'` characters or `'bad'` characters?\n",
    "3. Which publisher has a greater proportion of `'bad'` characters â€“ `'Marvel Comics'` or `'DC Comics'`?\n",
    "4. Which `'Publisher'` that isn't `'Marvel Comics'` or `'DC Comics'` has the most characters? Consider all characters whose `'Publisher'` we know â€“ that is, don't drop rows because they have null values in other columns.\n",
    "5. There is only one character that is **both** more one standard deviation above the mean in height and more than one standard deviation below the mean in weight. What is their name?\n",
    "\n",
    "***Note***: When calculating your answers, do not drop rows based on null values.\n",
    "\n",
    "***Note***: Although you'll be writing code to find the answers, you should not include your code in your `.py` file. Just return a hard-coded list with your answers to the 6 questions; all 6 elements in the list should be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Onslaught', 'George Lucas', 'bad', 'Marvel Comics', 'NBC - Heroes', 'Groot']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_answers = []\n",
    "\n",
    "q_0 = clean_out[(clean_out['Race']=='Mutant') & (clean_out['Hair color']=='No Hair')].sort_values('Height',ascending=False)['name'].iloc[0]\n",
    "my_answers.append(q_0)\n",
    "\n",
    "# ------------------------------------------ Q1 -----------------------------------------------\n",
    "\n",
    "# find publishers and the number of heroes they have \n",
    "by_publisher = clean_out.groupby('Publisher').count()['name'] # series \n",
    "\n",
    "# filter by publishers that have more than 5\n",
    "publishers_5 = by_publisher[by_publisher > 5]\n",
    "\n",
    "# get name of publishers that have > 5 heroes \n",
    "by_publisher = publishers_5.index.tolist() # series --> list \n",
    "\n",
    "# filter original df by by_publisher and Race == human\n",
    "# then groupby Publisher once more and get a series \n",
    "inter_step = clean_out[(clean_out['Publisher'].isin(by_publisher)) & (clean_out['Race']=='Human')].groupby('Publisher').count()['name']\n",
    "\n",
    "# filter publishers that have > 5 heroes to include only those that are Human \n",
    "positions = publishers_5.index.isin(inter_step.index.tolist())\n",
    "total_publishers_5 = publishers_5.iloc[positions]\n",
    "\n",
    "# get proportion of publishers who have more than 5 characters and which publisher has the highest proportion of human characters\n",
    "# priority sort numerically by values (thanks to 'mergesort') then alphabetically by index \n",
    "q_1_prop = (inter_step / total_publishers_5).sort_index(ascending=True).sort_values(ascending=False,kind='mergesort')\n",
    "\n",
    "q_1 = q_1_prop.index[0]\n",
    "\n",
    "my_answers.append(q_1)\n",
    "\n",
    "# ----------------------------------------- Q2 --------------------------------------------------\n",
    "desired_align = ['good','bad']\n",
    "filtered_heightdf = clean_out.dropna(subset=['Height'])\n",
    "q_2 = filtered_heightdf[filtered_heightdf['Alignment'].isin(desired_align)].groupby('Alignment')['Height'].mean().idxmax()\n",
    "\n",
    "my_answers.append(q_2)\n",
    "\n",
    "# ----------------------------------------- Q3 ----------------------------------------------\n",
    "\n",
    "desired_publ = ['Marvel Comics', 'DC Comics']\n",
    "bad_series = clean_out[(clean_out['Publisher'].isin(desired_publ)) & (clean_out['Alignment']=='bad')].groupby('Publisher')['name'].count()\n",
    "total_series = clean_out[clean_out['Publisher'].isin(desired_publ)].groupby('Publisher')['name'].count()\n",
    "q_3 = (bad_series/total_series).sort_values(ascending=False).index[0]\n",
    "\n",
    "my_answers.append(q_3)\n",
    "\n",
    "# ----------------------------------------- Q4 ------------------------------------------------\n",
    "\n",
    "q_4 = (clean_out[~clean_out['Publisher'].isin(desired_publ)].dropna(subset=['Publisher'])\n",
    "       .groupby('Publisher')['name'].count().sort_values(ascending=False).index[0])\n",
    "\n",
    "my_answers.append(q_4)\n",
    "\n",
    "# ----------------------------------------- Q5 ------------------------------------------------\n",
    "\n",
    "new_df = clean_out.set_index('name')[['Height','Weight']]\n",
    "find_std = new_df.std(axis=0,skipna=True) # sum of rows per each column\n",
    "std_h = find_std.iloc[0]\n",
    "std_w = find_std.iloc[1]\n",
    "\n",
    "q_5 = new_df[(new_df['Height'] > (new_df['Height'].mean() + std_h)) & (new_df['Weight'] < (new_df['Weight'].mean() - std_w))].index[0]\n",
    "\n",
    "my_answers.append(q_5)\n",
    "\n",
    "my_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "stats_out = super_hero_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q6 results: All test cases passed!"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: High Potential Individuals ðŸ“ˆ\n",
    "\n",
    "Last year, the United Kingdom ðŸ‡¬ðŸ‡§ announced a new [\"High Potential Individual\" visa](https://www.lexology.com/library/detail.aspx?g=41fa64ec-9272-468c-bdcb-8002745a754f), which allows graduates of universities ranked in the Top 50 globally to move to the UK without a job lined up. This visa has been a subject of much debate, in part due to how much rankings play a role. (Rest assured, UCSD is on the list!)\n",
    "\n",
    "In this section, you will analyze a dataset of university rankings, collected from  [here](https://www.kaggle.com/datasets/mylesoneill/world-university-rankings?datasetId=) (though we have pre-processed and modified the original dataset for the purposes of this question). Our version of the dataset is stored in `data/universities_unified.csv`.\n",
    "\n",
    "Columns:\n",
    "* `'world_rank'`: world rank of the institution\n",
    "* `'institution'`: name of the institution\n",
    "* `'national_rank'`: rank within the nation, formatted as `'country, rank'`\n",
    "* `'quality_of_education'`: rank by quality of education\n",
    "* `'alumni_employment'`: rank by alumni employment\n",
    "* `'quality_of_faculty'`: rank by quality of faculty\n",
    "* `'publications'`: rank by publications\n",
    "* `'influence'`: rank by influence\n",
    "* `'citations'`: rank by number of citations\n",
    "* `'broad_impact'`: rank by broad impact\n",
    "* `'patents'`: rank by number of patents\n",
    "* `'score'`: overall score of the institution, out of 100\n",
    "* `'control'`: whether the university is public or private\n",
    "* `'city'`: city in which the institution is located\n",
    "* `'state'`: state in which the institution is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "There are (still) a few aspects of the dataset we need to clean before it's ready for analysis.\n",
    "\n",
    "#### `clean_universities`\n",
    "\n",
    "Complete the implementation of the function `clean_universities`, which takes in the raw rankings DataFrame and returns a cleaned DataFrame, cleaned according to the following information:\n",
    "\n",
    "- Some `'institution'` names contain `'\\n'` characters (e.g. `'University of California\\nSan Diego'`). Replace all instances of `'\\n'` with `', '` (a comma and a space) in the `'institution'` column.\n",
    "\n",
    "- Change the data type of the `'broad_impact'` column to `int`.\n",
    "\n",
    "* Split `'national_rank'` into two columns, `'nation'` and `'national_rank_cleaned'`, where:\n",
    "    * `'nation'` is the country (or its dependency) indicated in the first part of `'national_rank'`. \n",
    "        * Note that there are **3** countries that appear under different names for different schools. For all 3 of these countries, you should pick **the name that is longer** and use that name for every occurrence of the country. One of the 3 countries is **`'Czech Republic'`**, which also appears as **`'Czechia'`** â€“ since these refer to the same country and `'Czech Republic'` is longer, all instances of either name should be replaced with `'Czech Republic'`. You need to find the other 2 countries on your own. \n",
    "        * As is mentioned below, your function will only be tested on the DataFrame in `data/universities_unified.csv`, so you only need to change these 3 country names.\n",
    "    * `'national_rank_cleaned'` is the integer in the latter part of `'national_rank'`. Make sure that the data type of this column is `int`. \n",
    "    * Don't include the original `'national_rank'` column in the output DataFrame.\n",
    "* Create a Boolean column `'is_r1_public'`. This column should contain `True` if a university is public and classified as R1 and `False` otherwise. Treat `np.nan`s as False. **Note that in the raw DataFrame, a university is classified as R1 if and only if it has non-null values in all of the following columns: `'control'`, `'city'`, and `'state'`.**\n",
    "    - Read [this page](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States) to learn more about what it means for a university to be classified as R1.\n",
    "    \n",
    "**The only dataset your function will be tested on is `data/universities_unified.csv`; you don't need to worry about other hidden test sets.** In addition, please return a *copy* of the original DataFrame; don't modify the original.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we can do some basic exploration.\n",
    "\n",
    "#### `university_info`\n",
    "\n",
    "Complete the implementation of the function `university_info`, which takes in the **cleaned** DataFrame outputted by `clean_universities` and returns the following values in a list:\n",
    "* Among `'state'`s with three or more `'institution'`s in the dataset, the `'state'` whose universities have the lowest mean `'score'`.\n",
    "* The proportion of the `'institution'`s in the top 100 for which the `'quality of faculty'` ranking is also in the top 100.\n",
    "* The number of `'state'`s where at least 50% of the `'institution'`s are private (i.e. have an `'is_r1_public'` of `False`).\n",
    "* The lowest-ranked (worst) `'institution'` in the world, according to `'world_rank'`, that is the highest-ranked (best) university in its nation (i.e., it has a `'national_rank_cleaned'` of 1).\n",
    "\n",
    "You can assume there are no ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = Path('data') / 'universities_unified.csv'\n",
    "df = pd.read_csv(fp)\n",
    "df['institution'] = df['institution'].str.replace('\\n',', ')\n",
    "df['broad_impact'] = df['broad_impact'].astype(int)\n",
    "\n",
    "# split 'national_rank' column\n",
    "df['nation'] = df['national_rank'].str.split(', ').str[0]\n",
    "df['national_rank_cleaned'] = df['national_rank'].str.split(', ').str[1]\n",
    "df['national_rank_cleaned'] = df['national_rank_cleaned'].astype(int)\n",
    "\n",
    "# df['nation'].unique() \n",
    "# UK and United Kingdom; USA and United States; Czechia and Czech Republic \n",
    "df['nation'] = df['nation'].replace(['UK','USA','Czechia'],['United Kingdom','United States','Czech Republic'])\n",
    "df = df.drop('national_rank',axis=1)\n",
    "\n",
    "df['is_r1_public'] = (\n",
    "    (df['control']=='Public') &\n",
    "    df['city'].notna() &\n",
    "    df['state'].notna()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OH'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------- Part 1 ------------------------------------------------\n",
    "states_df = df.groupby('state').count()[['institution']]\n",
    "states_series = (states_df[states_df['institution'] > 3])['institution']\n",
    "states_3 = states_series.index.tolist()\n",
    "\n",
    "lowest_score = (df[df['state'].isin(states_3)]).groupby('state')['score'].mean().sort_values(ascending=True).index[0]\n",
    "\n",
    "# ---------------------------------------- Part 2 -----------------------------------------------\n",
    "mini_df = df.set_index('institution')[['quality_of_faculty','world_rank']]\n",
    "one_filter_df = mini_df[(mini_df['world_rank'] <= 100)]\n",
    "both_filters_df = mini_df[(mini_df['world_rank'] <= 100) & (mini_df['quality_of_faculty'] <= 100)]\n",
    "\n",
    "prop_ranking = both_filters_df.shape[0]/one_filter_df.shape[0]\n",
    "# ---------------------------------------- Part 3 ----------------------------------------------\n",
    "mini_df2 = df.groupby(['state','is_r1_public'])[['institution']].count().reset_index(level='is_r1_public')\n",
    "private_states_series = mini_df2[mini_df2['is_r1_public']==False]['institution']\n",
    "\n",
    "total_state_series = df[df['state'].isin(private_states_series.index.tolist())].groupby('state')['institution'].count()\n",
    "\n",
    "num_states = np.count_nonzero((private_states_series/total_state_series).values >= 0.5)\n",
    "# --------------------------------------- Part 4 ----------------------------------------------\n",
    "mini_df3 = df.set_index('institution')[['world_rank','national_rank_cleaned']]\n",
    "mini_df3 = mini_df3.sort_values(['world_rank','national_rank_cleaned'],ascending=[False,True])\n",
    "top_national_df = mini_df3[mini_df3['national_rank_cleaned']==1]\n",
    "\n",
    "lowest_in_its_nation = top_national_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data') / 'universities_unified.csv'\n",
    "df = pd.read_csv(fp)\n",
    "cleaned = clean_universities(df)\n",
    "info = university_info(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q7</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q7 results: All test cases passed!"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done Lab 2! ðŸ\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.\n",
    "\n",
    "Once you've finished the lab, you should open the command line and run, in the directory for this lab:\n",
    "\n",
    "```\n",
    "python lab-validation.py\n",
    "```\n",
    "\n",
    "**This will run all of the `grader.check` cells that you see in this notebook, but only using the code in `lab.py` â€“ that is, it doesn't look at any of the code in this notebook. If all of your `grader.check` cells pass in this notebook but not all of them pass in your command line with the above command, then you likely have code in your notebook that isn't in your `lab.py`!**\n",
    "\n",
    "You can also use `lab-validation.py` to test individual questions. For instance,\n",
    "\n",
    "```\n",
    "python lab-validation.py q1 q4 q7\n",
    "```\n",
    "\n",
    "will run the `grader.check` cells for Questions 1, 4, and 7 â€“ again, only using the code in `lab.py`. [This video](https://www.loom.com/share/0ea254b85b2745e59322b5e5a8692e91?sid=5acc92e6-0dfe-4555-9b6a-8115b6a52f99) how to use the script as well.\n",
    "\n",
    "Once `python lab-validation.py` shows that you're passing all test cases, you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Remember, the only question in this lab that you're allowed to use a loop in is Question 3.</b> There, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame, but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> ans =  trick_me()\n>>> ans == 1 or ans == 2 or ans == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(trick_ans, list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(trick_ans[1], int)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out_pop.index.tolist() == ['A', 'B', 'C', 'D']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> cols = ['num_nonnull', 'prop_nonnull', 'num_distinct', 'prop_distinct']\n>>> out_pop.columns.tolist() == cols\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool((out_pop['num_distinct'] <= 10).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool((out_pop['prop_nonnull'] == 1.0).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> common_out.index.tolist() == [0, 1, 2]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(common_out['A_values'].isin(range(10)).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(super_out, list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(super_out) == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all([isinstance(x, str) for x in super_out])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> sum(clean_out['Skin color'].isnull()) > sum(heroes['Skin color'].isnull())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> sum(clean_out['Weight'].isnull()) > sum(heroes['Weight'].isnull())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(stats_out[0], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[1], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> stats_out[2] in ['good', 'bad']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> stats_out[3] in ['Marvel Comics', 'DC Comics']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[4], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[5], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cleaned.shape[0] == df.shape[0]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> cleaned['nation'].nunique() == 59\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(info) == 4\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> all([True if isinstance(x,y) else x.dtype==np.dtype(y) for x, y in zip(info, [str, float, int, str])])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> (info[1] >= 0) & (info[1] <= 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
